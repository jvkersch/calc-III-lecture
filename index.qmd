---
title: "How calculus powers machine learning and AI"
author: "Joris Vankerschaver (Ghent University Global Campus)"
format: revealjs
---

## About this presentation

- Presentation: [https://jvkersch.github.io/gmu-calc-lecture](https://jvkersch.github.io/gmu-calc-lecture)
- Code: [https://github.com/jvkersch/gmu-calc-lecture](https://github.com/jvkersch/gmu-calc-lecture)

- The presentation can be run from your browser
- Embedded scripts take a while to load (10-20s) first time

## Introduction

- Since 2021: professor at GUGC
  - Introduction to Engineering Mathematics
  - Probability and Statistics
  - Introduction to Statistical Modelling
- 2013-2021: Scientific software developer at Enthought (Cambridge, UK)
- Before 2013: Researcher in US, UK, Japan

## Professional experience

- Software for new electron microscopes
- DNA sequencing via a new electro/biological substrate
- Data/compute management for life sciences companies
- Predictive algorithms in oil and gas (subsoil modelling)
- Compound modelling in pharma

## How have I found calculus useful

::: {.incremental}
- I used to think that nobody cares about ODEs, integrals, vector fields, etc
- That is **categorically false**
- You have to do the job of **translating real-world problems** into calculus terms
:::

## What are we going to do today?

::: {.incremental}
- Your professors have demystified calculus for you
- We will try to demystify some **applications of calculus**
- We will use calculus (and a little linear algebra) to
  - Optimize neural networks and other functions
  - Find edges in images
:::

# Optimizing neural networks (and other functions)

## Example 1: Training a neural network

You have:

- Training data: $(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)$
- A model with parameters: $\theta = (\theta_1, \theta_2, \ldots, \theta_p)$
- A loss function: $L(\theta) = \frac{1}{n}\sum_{i=1}^n (f(x_i; \theta) - y_i)^2$

**Goal**: Find $\theta^*$ that minimizes $L(\theta)$

## Finding the minimum can be difficult

![](./images/nn-loss-landscape.png)

::: {.tiny-text}
[Visualizing the Loss Landscape of Neural Nets](https://arxiv.org/abs/1712.09913), Li et al., NeurIPS 2018.
:::

## Example 2: Profit maximization

A firm produces and sells a product.

- $q$: quantity produced  
- $P(q)$: price per unit  
- $C(q)$: cost function  

Maximize (possibly subject to constraints)
$$
\Pi(q) = P(q)q - C(q)
$$

Maximizing $\Pi(q)$ is the same as minimizing $L(q) = - \Pi(q)$.

## The calculus connection

To minimize $L(\theta)$, find zeros of the gradient

$$
    \nabla L(\theta) = 
        \begin{pmatrix}
            \frac{\partial L}{\partial \theta_1} \\ 
            \frac{\partial L}{\partial \theta_2} \\ 
            \vdots \\ 
            \frac{\partial L}{\partial \theta_p} 
        \end{pmatrix}
$$

- Zeros are minima, maxima, or saddle points
- Hard (impossible) to solve analytically

##

::: {.column-screen}
<iframe src="./second-derivative/" style="position:absolute; top:0; left:0; width:100%; height:100%;">
</iframe>
:::

## The descent beckons

:::: {.columns}
::: {.column width="50%"}
- You want to go down the mountain into the valley as efficiently as possible.
- The fog prevents you from seeing more than a few meters in every direction.
- How do you proceed?

üí° Walk in the direction of **steepest descent**
:::

::: {.column width="50%"}
![](./images/cdf-wanderer.jpeg){fig-align="center" width="75%"}

::: {.tiny-text}
[Wanderer above the Sea of Fog](https://en.wikipedia.org/wiki/Wanderer_above_the_Sea_of_Fog), Caspar David Friedrich (1818)
:::

:::
::::

## Moving in the direction of steepest descent

:::: {.columns}
::: {.column width="50%"}

Recall: 

- Gradient: direction of steepest ascent ‚¨ÜÔ∏è
- Negative gradient: **steepest descent** ‚¨áÔ∏è

:::

::: {.column width="50%"}
```{python}
%run scripts/gradient.py
```
:::
::::

## The gradient descent method

- Start with initial guess $\theta^{(0)}$ and iterate:
$$
    \theta^{(k+1)} = \theta^{(k)} - \alpha \nabla L(\theta^{(k)})
$$
Here, $\alpha$ is the **learning rate** (step size)

- Repeat until no more progress

##

::: {.column-screen}
<iframe src="./gradient-descent/" style="position:absolute; top:0; left:0; width:100%; height:100%;">
</iframe>
:::

## Aside: "continuous" gradient descent

:::: {.columns}
::: {.column width="50%"}
In the limit of very small steps ($\alpha \to 0$), we get an ODE

$$
    \frac{d {\theta}}{dt} = - \nabla L(\theta).
$$

Finding minima is the same as finding attracting fixed points of this ODE.
:::

::: {.column width="50%"}
```{python}
%run "scripts/ode.py"
```
:::
::::

## Backpropagation: gradient of a neural network

![](./images/Backpropagation-in-Neural-Network-1.webp){fig-align="center"}

::: {.tiny-text}
Source: GeeksForGeeks [Backpropagation in neural networks](https://www.geeksforgeeks.org/machine-learning/backpropagation-in-neural-network/)
:::

## Neural network training

(Stochastic) Gradient Descent:

-  How neural networks were trained up until the 2010s
-  Still a good algorithm to know about!

These days, more sophisticated algorithms exist (Adam, RMSprop, etc).

## Neural network training

![](./images/alexnet.png){fig-align="center"}

::: {.tiny-text}
Source: ImageNet Classification with Deep Convolutional
Neural Networks, Krizhevsky et al, NeurIPS (2012)
:::

## Neural network training

![](./images/2024-Alan-D-Thompson-Compute-Rev-9.png){fig-align="center"}

## Taking curvature into account

![](./images/ubehebe.jpeg)

## The Hessian matrix

$$
H(L)(\theta) =
\begin{pmatrix}
    \frac{\partial^2 L}{\partial \theta_1^2} & \cdots & \frac{\partial^2 L}{\partial \theta_1\partial \theta_n} \\
    \vdots & & \vdots \\
    \frac{\partial^2 L}{\partial \theta_n\partial \theta_1} & \cdots & \frac{\partial^2 L}{\partial \theta_n^2}
\end{pmatrix}
$$

- **Symmetric** if $L$ twice continuously differentiable.
- Captures **local curvature**: how the gradient changes near a point.

## Newton's method

- Gradient descent uses only first derivatives.
- **Newton‚Äôs method** uses curvature:

$$
\theta_{k+1} = \theta_k - H(L)^{-1}\nabla L(\theta_k)
$$


- Adapts step *magnitude* and *direction*.
- Quadratic convergence near a minimum if $H(L)$ is positive definite.

## Newton's method in action

```{python}
#| echo: false
%run scripts/convergence.py
```

## Conclusion so far

::: {.incremental}
- Neural networks are functions
- Finding optimal parameters = high-dimensional optimization
- Gradient/Hessian tells us how to adjust parameters
- Modern deep learning uses sophisticated variants (Adam, RMSprop)
- Calculus gives you the foundation to understand all of these algorithms!
:::


# Detecting edges in images

## The 10,000 foot view

:::: {.columns}
::: {.column width="60%"}

Main idea üí°

- Edges are where an image **suddenly changes**
- Changes can be quantified with derivatives

To detect edges in an image:

- Calculate its gradient
- Determine the magnitude of the gradient

:::
::: {.column width="40%"}
![](images/Valve_original.png){fig-align="center" width="75%"}
![](images/Valve_monochrome_canny.png){fig-align="center" width="75%"}

::: {.tiny-text}
Image credit: Wikipedia [Canny edge detector](https://en.wikipedia.org/wiki/Canny_edge_detector)
:::
:::
::::

## Images as functions

Images are just **functions** that assign a value to each pixel:

:::: {.columns}
::: {.column width="50%"}

Grayscale intensity

![](images/chelsea-gray.png){height="75%"}
$$
    I: [0, W] \times [0, H] \to \mathbb{R}^\phantom{3}
$$

:::
::: {.column width="50%"}
RGB(A) color

![](images/chelsea-color.png){height="75%"}
$$
    I: [0, W] \times [0, H] \to \mathbb{R}^3
$$
:::

::::

::: {.tiny-text}
Image credit: [Chelsea the cat](https://scikit-image.org/docs/0.23.x/api/skimage.data.html#skimage.data.cat), scikit-image developers (CC0)
:::

##

::: {.column-screen}
<iframe src="./image-as-function/" style="position:absolute; top:0; left:0; width:100%; height:100%;">
</iframe>
:::

## Detecting changes via gradients

If we have a function $I$, we can compute its gradient:
$$
\nabla I = \begin{pmatrix} 
        \frac{\partial I}{\partial x} \\
        \frac{\partial I}{\partial y}
    \end{pmatrix}.
$$

The magnitude
$$
    \left\Vert \nabla I \right\Vert =
    \sqrt{\left(\frac{\partial I}{\partial x}\right)^2 +
          \left(\frac{\partial I}{\partial y}\right)^2}
$$
tells us about **changes** in $I$.

## Images are discrete functions

![](images/chelsea-pixels-composite.svg)

Hard to compute the gradient of a discrete function!

## Think about rate of change

- The partial derivative tells us something about **rate of change**:
$$
    \frac{\partial I}{\partial x}: \text{rate of change in $x$-direction}
$$

- For a discrete function: replace
$$
    \frac{\partial I}{\partial x}(i, j) \rightarrow \boxed{I(i + 1, j) - I(i - 1, j)}
$$

::: {.tiny-text}
You can use Taylor expansions to show that the so-called central difference approximation above is proportional to the derivative up to second order, but as it's close to lunchtime we will not do that...
:::

## Finite differences are just matrices

Let's write our finite difference as a matrix multiplication:
\begin{align*}
    I_{i + 1, j} - I_{i - 1, j} & = \begin{bmatrix} -1 & 0 & 1 \end{bmatrix}
    \begin{bmatrix}
        I_{i-1, j} \\ 
        I_{i,j} \\
        I_{i+1, j}
    \end{bmatrix}
\end{align*}

Why would we do that? Computers these days are **very good** at working with matrices.

## Let's go one step further

To have a more stable rate of change, it makes sense to average over nearby pixels.

This can be written as a **convolution operation**:
$$
     \frac{\partial I}{\partial x} \rightarrow 
        \begin{bmatrix}
            -1 & 0 & 1 \\
            -2 & 0 & 2 \\
            -1 & 0 & 1 \\
        \end{bmatrix} \ast I
$$
Congratulations, you have just rediscovered the Sobel operator!

##

::: {.column-screen}
<iframe src="./edge-detector/" style="position:absolute; top:0; left:0; width:100%; height:100%;">
</iframe>
:::

## What does this give us?

The Sobel operator is:

- The basis for the **Canny edge detector**
- An example of a feature map in a **convolutional neural network**

![](images/Typical_cnn.png){fig-align="center"}

::: {.tiny-text}
Image credit: Wikipedia [Convolutional neural network](https://en.wikipedia.org/wiki/File:Typical_cnn.png)
:::

## Current state of the art

- The Canny edge detector is very efficient and easy to implement: state of the art for many years.
- Modern deep neural networks outperform it, but are much more expensive to run.

![](https://github.com/facebookresearch/sam2/raw/main/assets/sa_v_dataset.jpg?raw=true){fig-align="center" width="60%"}

::: {.tiny-text}
Image credit: Meta AI [SAM2](https://github.com/facebookresearch/sam2)
:::

# To wrap up

## Why is advanced calculus useful?

It's the bridge between:

::: {.incremental}
- Classical calculus ‚Üí Modern applied mathematics
- Single-variable thinking ‚Üí Multivariate reality
- Simple models ‚Üí Rich, realistic systems
- Theory ‚Üí Computational practice
:::

## What I found useful

If you take calculus:

::: {.incremental}
- Practice visualization (use Python, Mathematica, ...)
- Connect to your field (find data science applications)
- Don't fear dimensions (intuition from 2D/3D extends)
- Embrace computation (numerical methods are powerful)
:::

## Thank you for your attention!

- **Contact:** [joris.vankerschaver@ghent.ac.kr](mailto:joris.vankerschaver@ghent.ac.kr)
- **Presentation:** [https://jvkersch.github.io/gmu-calc-lecture](https://jvkersch.github.io/gmu-calc-lecture)
- **Code:** [https://github.com/jvkersch/gmu-calc-lecture](https://github.com/jvkersch/gmu-calc-lecture)


![](./images/qr-presentation.png){fig-align="center" width="50%"}