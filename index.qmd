---
title: "How calculus powers AI and modern tech"
author: "Joris Vankerschaver"
format: revealjs
---

## Who am I

# Optimizing mathematical functions

## Example 1: Training a neural network

You have:

- Training data: $(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)$
- A model with parameters: $\theta = (\theta_1, \theta_2, \ldots, \theta_p)$
- A loss function: $L(\theta) = \frac{1}{n}\sum_{i=1}^n (f(x_i; \theta) - y_i)^2$

**Goal**: Find $\theta^*$ that minimizes $L(\theta)$

## Finding the minimum can be difficult

![](./images/nn-loss-landscape.png)

::: {.tiny-text}
[Visualizing the Loss Landscape of Neural Nets](https://arxiv.org/abs/1712.09913), Li et al., NeurIPS 2018.
:::

## Example 2: Profit maximization

A firm produces and sells a product.

- $q$: quantity produced  
- $P(q)$: price per unit  
- $C(q)$: cost function  

Maximize (possibly subject to constraints)
$$
\Pi(q) = R(q) - C(q) = P(q)q - C(q)
$$

Maximizing $\Pi(q)$ is the same as minimizing $L(q) = - \Pi(q)$.

## The calculus connection

To minimize $L(\theta)$:

$$
    \nabla L(\theta) = 
        \begin{pmatrix}
            \frac{\partial L}{\partial \theta_1} \\ 
            \frac{\partial L}{\partial \theta_2} \\ 
            \vdots \\ 
            \frac{\partial L}{\partial \theta_p} 
        \end{pmatrix} = 0.
$$

::: {.incremental}
- This is a **gradient** (generalization of derivative)
- In practice: $p$ can be millions!
- Can't solve analytically ‚Üí use **gradient descent**
:::

## The descent beckons

:::: {.columns}
::: {.column width="50%"}
- You want to go down the mountain into the valley as efficiently as possible.
- The fog prevents you from seeing more than a few meters in every direction.
- How do you proceed?

üí° Walk in the direction of **steepest descent**
:::

::: {.column width="50%"}
![](./images/cdf-wanderer.jpeg){fig-align="center" width="75%"}

::: {.tiny-text}
[Wanderer above the Sea of Fog](https://en.wikipedia.org/wiki/Wanderer_above_the_Sea_of_Fog), Caspar David Friedrich (1818)
:::

:::
::::

## Moving in the direction of steepest descent

Recall: 

- Gradient: direction of steepest ascent ‚¨ÜÔ∏è
- Negative gradient: **steepest descent** ‚¨áÔ∏è

Start with initial guess $\theta^{(0)}$ and iterate:

$$\theta^{(k+1)} = \theta^{(k)} - \alpha \nabla L(\theta^{(k)})$$

Here, $\alpha$ is the **learning rate** (step size)

##

::: {.column-screen}
<iframe src="./gradient-descent/" style="position:absolute; top:0; left:0; width:100%; height:100%;">
</iframe>
:::

## The ODE perspective

In the limit of very small steps ($\alpha \to 0$), we get an ODE

$$
    \frac{d {\theta}}{dt} = - \nabla L(\theta).
$$

Finding minima is the same as finding attracting fixed points of this ODE.

```{python}
import numpy as np
import matplotlib.pyplot as plt

def himmelblau(x, y):
    return (x**2 + y - 11) ** 2 + (x + y**2 - 7) ** 2

def himmelblau_gradient(x, y):
    return [
        4 * (x**2 + y - 11) * x + 2 * (x + y**2 - 7),
        2 * (x**2 + y - 11) + 4 * (x + y**2 - 7) * y,
    ]

x = y = np.linspace(-6, 6, 100)
X, Y = np.meshgrid(x, y)
Z = himmelblau(X, Y)
U, V = himmelblau_gradient(X, Y)

plt.streamplot(X, Y, -U, -V, color=np.log(Z))
```

## Taking curvature into account

![](./images/ubehebe.jpeg)

## The Hessian matrix

$$
H(f)(x) =
\begin{pmatrix}
    \frac{\partial^2 f}{\partial x_1^2} & \cdots & \frac{\partial^2 f}{\partial x_1\partial x_n} \\
    \vdots & & \vdots \\
    \frac{\partial^2 f}{\partial x_n\partial x_1} & \cdots & \frac{\partial^2 f}{\partial x_n^2}
\end{pmatrix}
$$

- **Symmetric** if $f$ twice continuously differentiable.
- Captures **local curvature**: how the gradient changes near a point.

## Newton's method

- Gradient descent uses only first derivatives.
- **Newton‚Äôs method** uses curvature:

$$
\theta_{k+1} = \theta_k - H(L)^{-1}\nabla L(\theta_k)
$$


- Adapts step *magnitude* and *direction*.
- Quadratic convergence near a minimum if $H(f)$ is positive definite.

## Newton's method in action

```{python}
#| echo: false
%run scripts/convergence.py
```


# Detecting edges in images

## The 10,000 foot view

:::: {.columns}
::: {.column width="60%"}

Main idea üí°

- Edges are where an image **suddenly changes**
- Changes can be quantified with derivatives

To detect edges in an image:

- Calculate its gradient
- Determine the magnitude of the gradient

:::
::: {.column width="40%"}
![](images/Valve_original.png)
![](images/Valve_monochrome_canny.png)

::: {.small-text}
Image credit: Wikipedia [Canny edge detector](https://en.wikipedia.org/wiki/Canny_edge_detector)
:::
:::
::::

## How do we compute the gradient of an image?

Images are just **functions** that assign a value to each pixel:

:::: {.columns}
::: {.column width="50%"}

Grayscale intensity

![](images/chelsea-gray.png)
$$
    I: [0, W] \times [0, H] \to \mathbb{R}^\phantom{3}
$$

:::
::: {.column width="50%"}
RGB(A) color

![](images/chelsea-color.png)
$$
    I: [0, W] \times [0, H] \to \mathbb{R}^3
$$
:::

::::

::: {.small-text}
Image credit: [Chelsea the cat](https://scikit-image.org/docs/0.23.x/api/skimage.data.html#skimage.data.cat), scikit-image developers (CC0)
:::

##

::: {.column-screen}
<iframe src="./image-as-function/" style="position:absolute; top:0; left:0; width:100%; height:100%;">
</iframe>
:::

## Detecting changes via gradients

If we have a function $I$, we can compute its gradient:
$$
\nabla I = \begin{pmatrix} 
        \frac{\partial I}{\partial x} \\
        \frac{\partial I}{\partial y}
    \end{pmatrix}.
$$

The magnitude
$$
    \left\Vert \nabla I \right\Vert =
    \sqrt{\left(\frac{\partial I}{\partial x}\right)^2 +
          \left(\frac{\partial I}{\partial y}\right)^2}
$$
tells us about **changes** in $I$.

## Images are discrete functions

![](images/chelsea-pixels-composite.svg)

Hard to compute the gradient of a discrete function!

## Think about rate of change

- The partial derivative tells us something about **rate of change**:
$$
    \frac{\partial I}{\partial x}: \text{rate of change in $x$-direction}
$$

- For a discrete function: replace
$$
    \frac{\partial I}{\partial x}(i, j) \rightarrow \boxed{I(i + 1, j) - I(i - 1, j)}
$$

::: {.tiny-text}
You can use Taylor expansions to show that the so-called central difference approximation above is proportional to the derivative up to second order, but as it's close to lunchtime we will not do that...
:::

## Finite differences are just matrices

Let's write our finite difference as a matrix multiplication:
\begin{align*}
    I_{i + 1, j} - I_{i - 1, j} & = \begin{bmatrix} -1 & 0 & 1 \end{bmatrix}
    \begin{bmatrix}
        I_{i-1, j} \\ 
        I_{i,j} \\
        I_{i+1, j}
    \end{bmatrix}
\end{align*}

Why would we do that? Computers these days are **very good** at working with matrices.

## Let's go one step further

To have a more stable rate of change, it makes sense to average over nearby pixels.

This can be written as a **convolution operation**:
$$
     \frac{\partial I}{\partial x} \rightarrow 
        \begin{bmatrix}
            -1 & 0 & 1 \\
            -2 & 0 & 2 \\
            -1 & 0 & 1 \\
        \end{bmatrix} \ast I
$$
Congratulations, you have just rediscovered the Sobel operator!

##

::: {.column-screen}
<iframe src="./edge-detector/" style="position:absolute; top:0; left:0; width:100%; height:100%;">
</iframe>
:::

## What does this give us?

The Sobel operator is:

- The basis for the Canny edge detector
- An example of a feature map in a **convolutional neural network**

![](images/Typical_cnn.png){fig-align="center"}

::: {.small-text}
Image credit: Wikipedia [Convolutional neural network](https://en.wikipedia.org/wiki/File:Typical_cnn.png)
:::

## Current state of the art

- The Canny edge detector is very efficient and easy to implement: state of the art for many years.
- Modern deep neural networks outperform it, but are much more expensive to run.

![](https://github.com/facebookresearch/sam2/raw/main/assets/sa_v_dataset.jpg?raw=true){fig-align="center" width="60%"}

::: {.small-text}
Image credit: Facebook Research [SAM2](https://github.com/facebookresearch/sam2)
:::