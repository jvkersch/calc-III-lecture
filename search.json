[
  {
    "objectID": "index.html#who-am-i",
    "href": "index.html#who-am-i",
    "title": "How calculus powers AI and modern tech",
    "section": "Who am I",
    "text": "Who am I"
  },
  {
    "objectID": "index.html#the-10000-foot-view",
    "href": "index.html#the-10000-foot-view",
    "title": "How calculus powers AI and modern tech",
    "section": "The 10,000 foot view",
    "text": "The 10,000 foot view\n\n\nMain idea ðŸ’¡\n\nEdges are where an image suddenly changes\nChanges can be quantified with derivatives\n\nTo detect edges in an image:\n\nCalculate its gradient\nDetermine the magnitude of the gradient\n\n\n \n\nImage credit: Wikipedia Canny edge detector"
  },
  {
    "objectID": "index.html#how-do-we-compute-the-gradient-of-an-image",
    "href": "index.html#how-do-we-compute-the-gradient-of-an-image",
    "title": "How calculus powers AI and modern tech",
    "section": "How do we compute the gradient of an image?",
    "text": "How do we compute the gradient of an image?\nImages are just functions that assign a value to each pixel:\n\n\nGrayscale intensity\n \\[\n    I: [0, W] \\times [0, H] \\to \\mathbb{R}^\\phantom{3}\n\\]\n\nRGB(A) color\n \\[\n    I: [0, W] \\times [0, H] \\to \\mathbb{R}^3\n\\]\n\n\nImage credit: Chelsea the cat, scikit-image developers (CC0)"
  },
  {
    "objectID": "index.html#detecting-changes-via-gradients",
    "href": "index.html#detecting-changes-via-gradients",
    "title": "How calculus powers AI and modern tech",
    "section": "Detecting changes via gradients",
    "text": "Detecting changes via gradients\nIf we have a function \\(I\\), we can compute its gradient: \\[\n\\nabla I = \\begin{pmatrix}\n        \\frac{\\partial I}{\\partial x} \\\\\n        \\frac{\\partial I}{\\partial y}\n    \\end{pmatrix}.\n\\]\nThe magnitude \\[\n    \\left\\Vert \\nabla I \\right\\Vert =\n    \\sqrt{\\left(\\frac{\\partial I}{\\partial x}\\right)^2 +\n          \\left(\\frac{\\partial I}{\\partial y}\\right)^2}\n\\] tells us about changes in \\(I\\)."
  },
  {
    "objectID": "index.html#images-are-discrete-functions",
    "href": "index.html#images-are-discrete-functions",
    "title": "How calculus powers AI and modern tech",
    "section": "Images are discrete functions",
    "text": "Images are discrete functions\n\nHard to compute the gradient of a discrete function!"
  },
  {
    "objectID": "index.html#think-about-rate-of-change",
    "href": "index.html#think-about-rate-of-change",
    "title": "How calculus powers AI and modern tech",
    "section": "Think about rate of change",
    "text": "Think about rate of change\n\nThe partial derivative tells us something about rate of change: \\[\n  \\frac{\\partial I}{\\partial x}: \\text{rate of change in $x$-direction}\n\\]\nFor a discrete function: replace \\[\n  \\frac{\\partial I}{\\partial x}(i, j) \\rightarrow \\boxed{I(i + 1, j) - I(i - 1, j)}\n\\]\n\n\nYou can use Taylor expansions to show that the so-called central difference approximation above is proportional to the derivative up to second order, but as itâ€™s close to lunchtime we will not do thatâ€¦"
  },
  {
    "objectID": "index.html#finite-differences-are-just-matrices",
    "href": "index.html#finite-differences-are-just-matrices",
    "title": "How calculus powers AI and modern tech",
    "section": "Finite differences are just matrices",
    "text": "Finite differences are just matrices\nLetâ€™s write our finite difference as a matrix multiplication: \\[\\begin{align*}\n    I_{i + 1, j} - I_{i - 1, j} & = \\begin{bmatrix} -1 & 0 & 1 \\end{bmatrix}\n    \\begin{bmatrix}\n        I_{i-1, j} \\\\\n        I_{i,j} \\\\\n        I_{i+1, j}\n    \\end{bmatrix}\n\\end{align*}\\]\nWhy would we do that? Computers these days are very good at working with matrices."
  },
  {
    "objectID": "index.html#lets-go-one-step-further",
    "href": "index.html#lets-go-one-step-further",
    "title": "How calculus powers AI and modern tech",
    "section": "Letâ€™s go one step further",
    "text": "Letâ€™s go one step further\nTo have a more stable rate of change, it makes sense to average over nearby pixels.\nThis can be written as a convolution operation: \\[\n     \\frac{\\partial I}{\\partial x} \\rightarrow\n        \\begin{bmatrix}\n            -1 & 0 & 1 \\\\\n            -2 & 0 & 2 \\\\\n            -1 & 0 & 1 \\\\\n        \\end{bmatrix} \\ast I\n\\] Congratulations, you have just rediscovered the Sobel operator!"
  },
  {
    "objectID": "index.html#what-does-this-give-us",
    "href": "index.html#what-does-this-give-us",
    "title": "How calculus powers AI and modern tech",
    "section": "What does this give us?",
    "text": "What does this give us?\nThe Sobel operator is:\n\nThe basis for the Canny edge detector\nAn example of a feature map in a convolutional neural network\n\n\n\nImage credit: Wikipedia Convolutional neural network"
  },
  {
    "objectID": "index.html#current-state-of-the-art",
    "href": "index.html#current-state-of-the-art",
    "title": "How calculus powers AI and modern tech",
    "section": "Current state of the art",
    "text": "Current state of the art\n\nThe Canny edge detector is very efficient and easy to implement: state of the art for many years.\nModern deep neural networks outperform it, but are much more expensive to run.\n\n\n\nImage credit: Facebook Research SAM2"
  }
]